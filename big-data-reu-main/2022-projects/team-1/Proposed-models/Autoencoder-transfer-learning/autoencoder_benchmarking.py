# -*- coding: utf-8 -*-
"""Autoencoder benchmarking

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lz6y3-rD1IalxmtG_ibesgj58h7vqxNT
"""

# connects colab to your google drive
# skip if your dataset is not on google drive or you're not using colab
from google.colab import drive
drive.mount('/content/drive')

# import packages
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import keras
import os
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras import datasets, layers, models
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, BatchNormalization, Conv2DTranspose
from keras.preprocessing.image import ImageDataGenerator
import sklearn.metrics as metrics
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from scipy.stats import norm
from keras.constraints import UnitNorm, Constraint

filepath = 'put your dataset path here'
os.chdir(filepath) # changes the current working directory to the file path specified. This directory should be the directory of data you plan on using for the model

batch_size = 32 # set batch_size to 32 because it seems to provide the best results given our dataset

# this ImageDataGenerator is used for training, validation, and testing generators. No preprocessing besides dividing all values by 255 is carried out
train_datagen = ImageDataGenerator(
        rescale=1./255
        )

train = train_datagen.flow_from_directory(
        filepath + "train/",
        target_size=(256, 256), # images, regardless of their original size, are shrunk down to 256 x 256 x 3
        batch_size=batch_size,
        color_mode = 'rgb',
        class_mode='binary', # for binary classification
        shuffle=False # do not shuffle images
    )

val = train_datagen.flow_from_directory(
        filepath + "validation/",
        target_size=(256, 256),
        batch_size=batch_size,
        color_mode = 'rgb',
        class_mode='binary',
        shuffle=False
    )

test = train_datagen.flow_from_directory(
        filepath + "test/",
        target_size=(256, 256),
        batch_size=batch_size,
        color_mode = 'rgb',
        class_mode='binary',
        shuffle=False
    )

# this model has the exact same architecture and regularizers as the classification model, but without any pre-trained weights
# if images are not rgb, input_shape should be changed to (256, 256, 1) instead of (256, 256, 3)
baseline = models.Sequential()
baseline.add(layers.Conv2D(16, (3, 3), activation='relu', padding='same', input_shape = (256, 256, 3), kernel_regularizer = keras.regularizers.l2(l = 0.01)))
baseline.add(layers.MaxPooling2D((2, 2), padding='same'))
baseline.add(layers.Conv2D(8, (3, 3), activation='relu', padding='same', kernel_regularizer = keras.regularizers.l2(l = 0.01)))
baseline.add(layers.MaxPooling2D((2, 2), padding='same'))
baseline.add(layers.Conv2D(8, (3, 3), activation='relu', padding='same'))
baseline.add(layers.MaxPooling2D((2, 2), padding='same'))
baseline.add(keras.layers.Flatten())
baseline.add(keras.layers.Dense(64, activation='relu'))
baseline.add(keras.layers.Dropout(0.3))
baseline.add(keras.layers.Dense(1, activation='sigmoid'))

baseline.compile(
        optimizer='Adam',
        metrics=['accuracy'],
        loss='binary_crossentropy')

epochs = 100

h = baseline.fit(
    train,
    epochs=epochs,
    validation_data=val,
    verbose=1)

# plots training and validation accuracies
plt.plot(h.history['accuracy'])
plt.plot(h.history['val_accuracy'])
plt.title('Training and Validation Accuracies')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Training', 'Validation'], loc = 'upper left')
plt.show()

# plots training and validation losses
plt.plot(h.history['loss'])
plt.plot(h.history['val_loss'])
plt.title('Training and Validation Losses')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Training', 'Validation'], loc = 'upper left')
plt.show()

# evaludates test accuracy
_, acc = baseline.evaluate(test, verbose = 0)
print('> %.3f' % (acc * 100))

# gets confusion matrix for predictions on validation set
# can be changed to get confusion matrix for predictions on training or test sets as well
pred = baseline.predict(val)

print("Confusion Matrix: \n")
true_classes = val.classes
class_labels = list(val.class_indices.keys())
pred = np.round(pred)
confusion_matrix = metrics.confusion_matrix(y_true=true_classes, y_pred=pred)
confusion_matrix